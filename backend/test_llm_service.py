"""
Test script for LLM Service - Context Setup (Task 1)
Tests the extraction of criteria from rubric text using Gemini
"""
import asyncio
import sys
import os

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app.services.llm_service import llm_service
import json


async def test_basic_extraction():
    """Test basic criteria extraction"""
    print("\n" + "="*80)
    print("TEST 1: Basic Criteria Extraction")
    print("="*80)
    
    rubric_text = """
    B√†i essay ph·∫£i c√≥:
    - Thesis statement r√µ r√†ng
    - √çt nh·∫•t 3 lu·∫≠n ƒëi·ªÉm ch√≠nh v·ªõi d·∫´n ch·ª©ng c·ª• th·ªÉ
    - K·∫øt lu·∫≠n t√≥m t·∫Øt ƒë·∫ßy ƒë·ªß
    """
    
    print(f"\nRubric Text:\n{rubric_text}")
    print("\nExtracting...")
    
    result = await llm_service.extract_criteria_from_rubric(
        rubric_text=rubric_text,
        writing_type="Essay"
    )
    
    print("\nüìä Result:")
    print(json.dumps(result, indent=2, ensure_ascii=False))
    print(f"\n‚úÖ Extracted {len(result.get('criteria', []))} criteria")
    return result


async def test_technical_proposal():
    """Test technical proposal extraction"""
    print("\n" + "="*80)
    print("TEST 2: Technical Proposal Extraction")
    print("="*80)
    
    rubric_text = """
    ƒê·ªÅ xu·∫•t k·ªπ thu·∫≠t ph·∫£i ch·ª©ng minh NoSQL c√≥ kh·∫£ nƒÉng m·ªü r·ªông t·ªët h∆°n.
    Y√™u c·∫ßu:
    - C√≥ lu·∫≠n c·ª© k·ªπ thu·∫≠t v·ªÅ scalability
    - Ph√¢n t√≠ch chi ph√≠ so v·ªõi SQL
    - Timeline tri·ªÉn khai th·ª±c t·∫ø
    - So s√°nh performance v·ªõi RDBMS
    """
    
    constraints = "word_limit: 1000, deadline: 2 weeks"
    
    print(f"\nRubric Text:\n{rubric_text}")
    print(f"\nConstraints: {constraints}")
    print("\nExtracting...")
    
    result = await llm_service.extract_criteria_from_rubric(
        rubric_text=rubric_text,
        writing_type="Technical Proposal",
        key_constraints=constraints
    )
    
    print("\nüìä Result:")
    print(json.dumps(result, indent=2, ensure_ascii=False))
    
    # Analyze result
    criteria = result.get('criteria', [])
    print(f"\n‚úÖ Extracted {len(criteria)} criteria")
    
    if criteria:
        print("\nüìã Criteria Summary:")
        for c in criteria:
            mandatory = "üî¥ MANDATORY" if c.get('is_mandatory') else "‚ö™ Optional"
            weight = c.get('weight', 0)
            print(f"  {mandatory} [{weight:.1f}] {c.get('label')}")
            print(f"    ‚îî‚îÄ {c.get('description', '')[:100]}...")
    
    return result


async def test_validation():
    """Test criteria validation"""
    print("\n" + "="*80)
    print("TEST 3: Criteria Validation")
    print("="*80)
    
    test_criteria = [
        {
            "label": "Scalability demonstration",
            "description": "Ch·ª©ng minh NoSQL m·ªü r·ªông t·ªët",
            "weight": 1.0,
            "is_mandatory": True
        },
        {
            "label": "Cost analysis",
            "description": "Ph√¢n t√≠ch chi ph√≠",
            "weight": 0.7,
            "is_mandatory": False
        }
    ]
    
    print("\nCriteria to validate:")
    print(json.dumps(test_criteria, indent=2, ensure_ascii=False))
    print("\nValidating for 'Technical Proposal'...")
    
    result = await llm_service.validate_criteria_alignment(
        criteria=test_criteria,
        writing_type="Technical Proposal"
    )
    
    print("\nüìä Validation Result:")
    print(json.dumps(result, indent=2, ensure_ascii=False))
    
    if result.get('is_valid'):
        print("\n‚úÖ Criteria are valid")
    else:
        print("\n‚ö†Ô∏è Criteria need improvement")
    
    if result.get('suggestions'):
        print("\nüí° Suggestions:")
        for s in result['suggestions']:
            print(f"  - {s}")
    
    if result.get('missing_elements'):
        print("\n‚ùå Missing elements:")
        for m in result['missing_elements']:
            print(f"  - {m}")
    
    return result


async def test_edge_cases():
    """Test edge cases"""
    print("\n" + "="*80)
    print("TEST 4: Edge Cases")
    print("="*80)
    
    # Test 1: Very short rubric
    print("\nüìù Test 4.1: Very short rubric")
    result1 = await llm_service.extract_criteria_from_rubric(
        rubric_text="Vi·∫øt t·ªët v√† r√µ r√†ng.",
        writing_type="Blog Post"
    )
    print(f"‚úÖ Extracted {len(result1.get('criteria', []))} criteria from minimal text")
    
    # Test 2: Vietnamese + English mixed
    print("\nüìù Test 4.2: Mixed language rubric")
    result2 = await llm_service.extract_criteria_from_rubric(
        rubric_text="""
        Technical report must include:
        - Executive summary b·∫±ng ti·∫øng Vi·ªát
        - Data analysis v·ªõi charts
        - Recommendations d·ª±a tr√™n findings
        """,
        writing_type="Report"
    )
    print(f"‚úÖ Extracted {len(result2.get('criteria', []))} criteria from mixed language")
    
    # Test 3: Long detailed rubric
    print("\nüìù Test 4.3: Long detailed rubric")
    long_rubric = """
    B√†i pitch deck cho startup c·∫ßn:
    1. Problem statement: M√¥ t·∫£ v·∫•n ƒë·ªÅ th·ªã tr∆∞·ªùng m·ªôt c√°ch sinh ƒë·ªông, d√πng data ƒë·ªÉ support
    2. Solution: Gi·∫£i ph√°p ƒë·ªôc ƒë√°o, c√≥ demo/prototype n·∫øu c√≥ th·ªÉ
    3. Market size: TAM, SAM, SOM v·ªõi s·ªë li·ªáu c·ª• th·ªÉ
    4. Business model: R√µ r√†ng c√°ch ki·∫øm ti·ªÅn, pricing strategy
    5. Traction: Metrics, users, revenue (n·∫øu c√≥)
    6. Team: Background c·ªßa founders, advisors
    7. Competition: So s√°nh v·ªõi ƒë·ªëi th·ªß, competitive advantage
    8. Financial projections: 3-5 nƒÉm, realistic
    9. Ask: C·∫ßn bao nhi√™u v·ªën, d√πng v√†o ƒë√¢u
    """
    result3 = await llm_service.extract_criteria_from_rubric(
        rubric_text=long_rubric,
        writing_type="Pitch Deck",
        key_constraints="10-15 slides, 10 minutes presentation"
    )
    print(f"‚úÖ Extracted {len(result3.get('criteria', []))} criteria from detailed rubric")
    print(f"   Main goal: {result3.get('main_goal', 'N/A')[:80]}...")
    
    return result1, result2, result3


async def main():
    """Run all tests"""
    print("\n" + "="*80)
    print("üß™ LogicGuard LLM Service - Context Setup Tests")
    print("="*80)
    print("Testing Gemini 2.5 Flash for criteria extraction from rubric text")
    
    try:
        # Run tests
        await test_basic_extraction()
        await test_technical_proposal()
        await test_validation()
        await test_edge_cases()
        
        print("\n" + "="*80)
        print("‚úÖ ALL TESTS COMPLETED SUCCESSFULLY")
        print("="*80)
        print("\nNext steps:")
        print("1. Check if criteria are accurate and well-structured")
        print("2. Verify weights and mandatory flags are appropriate")
        print("3. Test with real rubrics from your use cases")
        print("4. Proceed to Step 3: Create Goal validation endpoints")
        
    except Exception as e:
        print("\n" + "="*80)
        print("‚ùå TEST FAILED")
        print("="*80)
        print(f"\nError: {str(e)}")
        print("\nPlease check:")
        print("1. GEMINI_API_KEY is set correctly in .env")
        print("2. API key is valid and active")
        print("3. Internet connection is working")
        import traceback
        print("\nFull traceback:")
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
